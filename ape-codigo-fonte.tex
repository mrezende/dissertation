%% ------------------------------------------------------------------------- %%
\chapter{Código-fonte dos modelos}
\label{ape:codigo-fonte-dos-modelos}

Neste capítulo apresentamos parte do código-fonte utilizado em nossos experimentos. O código-fonte completo está disponível no seguinte endereço: \url{https://github.com/mrezende/concra}. Neste repositório estão disponíveis o código-fonte para treinamento, avaliação dos modelos, juntamente com os dados originais e os dados pré-preprocessados. Além disso, também estão disponíveis o word2vec utilizado como entrada em nossos modelos. Utilizamos a linguagem Python, versão 3.6.9 com as bibliotecas \Gls{keras} (versão 2.2.4-tf) e \Gls{tensorflow} (1.15.2). Todos os nossos experimentos foram executados na plataforma \Gls{colab} do Google. Optamos por apresentar neste capítulo apenas os códigos-fontes dos modelos propostos para o presente trabalho. Os códigos-fontes do pré-processamento, treinamento e avaliação estão disponíveis publicamente em nosso repositório. Os dados estão sob a licença \href{https://creativecommons.org/licenses/by/4.0/}{Creative Commons} e os códigos estão disponíveis sob a licença \href{https://opensource.org/licenses/MIT}{MIT}.

\section{Código do modelo de referência \textit{Embedding}}
\label{sec:codigo-modelo-embedding}

O código a seguir é referente ao modelo que utilizamos como base para comparação em nossos experimentos. Este modelo é bem simples, ele recebe como entrada na primeira camada os vetores de representação distribuída das questões e trechos de código-fonte. E extrai as características mais relevantes dos vetores através de uma camada \textit{maxpool} (linha 22). E ao final calcula a distância de similaridade \textit{cosine} entre os vetores de representação da questão e do trecho de código-fonte. 

\begin{mypython-linenumber}{Embedding}
class EmbeddingModel(LanguageModel):
    def build(self):
        question = Input(shape=(self.question_len,), dtype='int32', name='question')
        answer = Input(shape=(self.answer_len,), dtype='int32', name='answer')

        # add embedding layers
        question_weights = np.load(self.config.initial_question_weights())
        q_embedding = Embedding(input_dim=question_weights.shape[0],
                                output_dim=question_weights.shape[1],
                                weights=[question_weights],
                                name='question_embedding')
        question_embedding = q_embedding(question)

        answer_weights = np.load(self.config.initial_answer_weights())
        a_embedding = Embedding(input_dim=answer_weights.shape[0],
                                output_dim=answer_weights.shape[1],
                                weights=[answer_weights],
                                name='answer_embedding')
        answer_embedding = a_embedding(answer)

        # maxpooling
        maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]),
                         name='max')
        maxpool.supports_masking = True
        question_pool = maxpool(question_embedding)
        answer_pool = maxpool(answer_embedding)
        
        cos_similarity = Lambda(lambda x: cosine_similarity(x[0], x[1], axis=1)
                                       , output_shape=lambda _: (None, 1), name='similarity')([question_pool,
                                                                                               answer_pool])

        return Model(inputs=[question, answer], outputs=cos_similarity,
                                   name='qa_model')

\end{mypython-linenumber}

\vspace{2cm}

\section{Código do modelo Unif}

O código a seguir apresenta a camada da questão do modelo Unif proposto por \cite{cambronero-deep-learning-code-search:2019}. O vetor de representação da questão é obtido através do cálculo da média aritmética simples dos vetores de representação de cada palavra que fazem parte da sentença (linha 32).

\begin{mypython-linenumber}{Camada da questão do modelo Unif}
class AverageLayer(Layer):
    def __init__(self, **kwargs):
        super(AverageLayer, self).__init__(**kwargs)

    def build(self, inputs_shape):
        inputs_shape = inputs_shape if isinstance(inputs_shape, list) else [inputs_shape]

        if len(inputs_shape) != 1:
            raise ValueError("AverageLayer expect one input.")

        # The first (and required) input is the actual input to the layer
        input_shape = inputs_shape[0]

        # Expected input shape consists of a triplet: (batch, input_length, input_dim)
        if len(input_shape) != 3:
            raise ValueError("Input shape for AverageLayer should be of 3 dimension.")

        self.input_length = int(input_shape[1])
        self.input_dim = int(input_shape[2])

        super(AverageLayer, self).build(input_shape)

    def call(self, inputs, **kwargs):
        inputs = inputs if isinstance(inputs, list) else [inputs]

        if len(inputs) != 1:
            raise ValueError("AverageLayer expect one input.")

        actual_input = inputs[0]

        # (batch, input_length, input_dim) = mean => (batch, input_dim)
        result = K.mean(actual_input, axis=1)

        return result

    def compute_output_shape(self, input_shape):
        return input_shape[0], input_shape[2] # (batch, input_dim)
\end{mypython-linenumber}

\vspace{2cm}

O código a seguir refere-se a camada de atenção utilizada para obter o vetor de representação final do trecho de código-fonte. A camada de atenção calcula uma média ponderada sobre os vetores de representação distribuída de cada palavra da sentença (linhas 50, 58, 59). E esta camada aprende ao longo do treinamento a ''prestar atenção'' (ponderar) as palavras mais importantes de acordo com o objetivo da tarefa. No nosso caso, o objetivo é aproximar os trechos de código-fonte anotados como corretos às suas questões.

\begin{mypython-linenumber}{Camada de atenção do modelo Unif}
# attention from https://github.com/tech-srl/code2vec
# paper: Code2Vec: Learning Distributed Representations of Code

class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, inputs_shape):
        inputs_shape = inputs_shape if isinstance(inputs_shape, list) else [inputs_shape]

        if len(inputs_shape) < 1 or len(inputs_shape) > 2:
            raise ValueError("AttentionLayer expect one or two inputs.")

        # The first (and required) input is the actual input to the layer
        input_shape = inputs_shape[0]

        # Expected input shape consists of a triplet: (batch, input_length, input_dim)
        if len(input_shape) != 3:
            raise ValueError("Input shape for AttentionLayer should be of 3 dimension.")

        self.input_length = int(input_shape[1])
        self.input_dim = int(input_shape[2])
        attention_param_shape = (self.input_dim, 1)

        self.attention_param = self.add_weight(
            name='attention_param',
            shape=attention_param_shape,
            initializer='uniform',
            trainable=True,
            dtype=tf.float32)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs, **kwargs):
        inputs = inputs if isinstance(inputs, list) else [inputs]

        if len(inputs) < 1 or len(inputs) > 2:
            raise ValueError("AttentionLayer expect one or two inputs.")

        actual_input = inputs[0]
        mask = inputs[1] if len(inputs) > 1 else None
        if mask is not None and not (((len(mask.shape) == 3 and mask.shape[2] == 1) or len(mask.shape) == 2)
                                     and mask.shape[1] == self.input_length):
            raise ValueError('mask should be of shape (batch, input_length)'
                             'or (batch, input_length, 1) '
                             'when calling an AttentionLayer.')

        assert actual_input.shape[-1] == self.attention_param.shape[0]

        # (batch, input_length, input_dim) * (input_dim, 1) ==> (batch, input_length, 1)
        attention_weights = K.dot(actual_input, self.attention_param)

        if mask is not None:
            if len(mask.shape) == 2:
                mask = K.expand_dims(mask, axis=2)  # (batch, input_length, 1)
            mask = K.log(mask)
            attention_weights += mask

        attention_weights = K.softmax(attention_weights, axis=1)  # (batch, input_length, 1)
        result = K.sum(actual_input * attention_weights, axis=1)  # (batch, input_length)  [multiplication uses broadcast]
        return result

    def compute_output_shape(self, input_shape):
        return input_shape[0], input_shape[2] # (batch, input_dim)
\end{mypython-linenumber}


\vspace{2cm}

O código a seguir refere-se ao modelo Unif. Ele recebe como parâmetro os vetores de representação distribuída das questões e trechos de código-fonte e, ao final, calcula a similaridade entre eles através da função \textit{cosine}. O vetor de representação da questão é obtido através da média aritmética simples dos vetores de cada palavra da sentença (linha 17). E os vetores de representação dos trechos de código-fonte são obtidos através da camada de atenção (linha 27).


\begin{mypython-linenumber}{Unif}
class UnifModel(LanguageModel):
    def build(self):
        print(tf.__version__)
        print(tf.keras.__version__)
        question = Input(shape=(self.question_len,), dtype='int32', name='question')
        answer = Input(shape=(self.answer_len,), dtype='int32', name='answer')

        # add embedding layers
        question_weights = np.load(self.config.initial_question_weights())
        q_embedding = Embedding(input_dim=question_weights.shape[0],
                                output_dim=question_weights.shape[1],
                                weights=[question_weights],
                                name='question_embedding')
        question_embedding = q_embedding(question)

        f_average_layer = AverageLayer(name='average')
        e_q = f_average_layer([question_embedding])

        answer_weights = np.load(self.config.initial_answer_weights())
        a_embedding = Embedding(input_dim=answer_weights.shape[0],
                                output_dim=answer_weights.shape[1],
                                weights=[answer_weights],
                                name='answer_embedding')
        answer_embedding = a_embedding(answer)

        f_attention_layer = AttentionLayer(name='attention')
        e_c = f_attention_layer([answer_embedding])
        
        cos_similarity = Lambda(lambda x: cosine_similarity(x[0], x[1], axis=1)
                                       , output_shape=lambda _: (None, 1), name='similarity')([e_q,
                                                                                               e_c])

        return Model(inputs=[question, answer], outputs=cos_similarity,
                                   name='qa_model')
\end{mypython-linenumber}
\vspace{2cm}
\section{Redes convolucionais}

Os códigos apresentados a seguir referem-se a rede convolucional proposta no Capítulo~\ref{cap:abordagem}. Estes códigos foram utilizados durante treinamento e avaliação dos nossos modelos. E uma adaptação foi necessária para comportar o experimento que avalia os diferentes tamanhos de janela do filtro convolucional. Em todos os códigos foram necessários o acréscimo do seguinte trecho para permitir a combinação de filtros convolucionais com diferentes tamanhos de janela:

\begin{mypython-linenumber}{Trecho para permitir filtros convolucionais com diferentes tamanhos de janela}
if len(kernel_size) > 1:
    q_cnns = [Conv1D(kernel_size=k,
                             filters=filters,
                             activation='relu',
                             padding='same',
                             name=f'question_conv1d_{k}') for k in kernel_size]
            # question_cnn = merge([cnn(question_embedding) for cnn in cnns], mode='concat')
            question_cnn = concatenate([cnn(question_embedding) for cnn in q_cnns])
...


else:
    k = kernel_size[0]
            q_cnn = Conv1D(kernel_size=k,
                             filters=filters,
                             activation='relu',
                             padding='same',
                             name=f'question_conv1d_{k}')
            # question_cnn = merge([cnn(question_embedding) for cnn in cnns], mode='concat')
            question_cnn = q_cnn(question_embedding)
\end{mypython-linenumber}

\vspace{2cm}
Na primeira condição na linha 1, verificamos se o vetor \emph{kernel\_size}, que indica o tamanho das janelas dos filtros convolucionais, possui mais de 1 tamanho de janela. Caso ele possua, criamos os filtros convolucionais para cada tamanho (linha 2) e concatenamos ao final (linha 8). Uma ilustração deste procedimento pode ser visualizado Figura~\ref{fig:cnn-architecture-proposal} do Capítulo~\ref{cap:abordagem}. Caso o vetor \emph{kernel\_size} possua apenas 1 tamanho de janela, não há necessidade de concatenar os filtros e apenas aplicamos a operação de convolução nos vetores de representação distribuída (linhas 12, 14, 20).

\subsection{Rede convolucional com parâmetros independentes entre as camadas de questão e trecho de código-fonte}
\label{sec:rede-convolucional-parametros-indepdentes-codigo-fonte-sem-normalizacao}
O código a seguir apresenta a rede convolucional no qual os parâmetros de aprendizagem da questão e trecho de código-fonte são independentes. Nas linhas 31 e 39 e nas linhas 47 e 55, criamos uma camada Conv1D para a questão e outra para o trecho de código-fonte. Com isto, não compartilhamos os parâmetros durante a aprendizagem de representação da questão e do trecho de código-fonte.

\begin{mypython-linenumber}{Rede convolucional com parâmetros independentes}
class ConvolutionModel(LanguageModel):
    def build(self):
        assert self.config.question_len() == self.config.answer_len()

        question = Input(shape=(self.question_len,), dtype='int32', name='question')
        answer = Input(shape=(self.answer_len,), dtype='int32', name='answer')

        # add embedding layers
        question_weights = np.load(self.config.initial_question_weights())
        q_embedding = Embedding(input_dim=question_weights.shape[0],
                                output_dim=question_weights.shape[1],
                                weights=[question_weights],
                                name='question_embedding')
        question_embedding = q_embedding(question)

        answer_weights = np.load(self.config.initial_answer_weights())
        a_embedding = Embedding(input_dim=answer_weights.shape[0],
                                output_dim=answer_weights.shape[1],
                                weights=[answer_weights],
                                name='answer_embedding')
        answer_embedding = a_embedding(answer)

        # cnn
        filters = self.config.filters()
        kernel_size = self.kernel_size

        question_cnn = None
        answer_cnn = None

        if len(kernel_size) > 1:
            q_cnns = [Conv1D(kernel_size=k,
                             filters=filters,
                             activation='relu',
                             padding='same',
                             name=f'question_conv1d_{k}') for k in kernel_size]
            # question_cnn = merge([cnn(question_embedding) for cnn in cnns], mode='concat')
            question_cnn = concatenate([cnn(question_embedding) for cnn in q_cnns])
            # answer_cnn = merge([cnn(answer_embedding) for cnn in cnns], mode='concat')
            a_cnns = [Conv1D(kernel_size=k,
                             filters=filters,
                             activation='relu',
                             padding='same',
                             name=f'answer_conv1d_{k}') for k in kernel_size]
            answer_cnn = concatenate([cnn(answer_embedding) for cnn in a_cnns])
        else:
            k = kernel_size[0]
            q_cnn = Conv1D(kernel_size=k,
                             filters=filters,
                             activation='relu',
                             padding='same',
                             name=f'question_conv1d_{k}')
            # question_cnn = merge([cnn(question_embedding) for cnn in cnns], mode='concat')
            question_cnn = q_cnn(question_embedding)
            # answer_cnn = merge([cnn(answer_embedding) for cnn in cnns], mode='concat')
            a_cnn = Conv1D(kernel_size=k,
                             filters=filters,
                             activation='relu',
                             padding='same',
                             name=f'answer_conv1d_{k}')
            answer_cnn = a_cnn(answer_embedding)

        # maxpooling
        maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]),
                         name='max')
        maxpool.supports_masking = True
        # enc = Dense(100, activation='tanh')
        # question_pool = enc(maxpool(question_cnn))
        # answer_pool = enc(maxpool(answer_cnn))
        question_pool = maxpool(question_cnn)
        answer_pool = maxpool(answer_cnn)

        cos_similarity = Lambda(lambda x: cosine_similarity(x[0], x[1], axis=1)
                                       , output_shape=lambda _: (None, 1), name='similarity')([question_pool,
                                                                                               answer_pool])

        return Model(inputs=[question, answer], outputs=cos_similarity,
                                   name='qa_model')
\end{mypython-linenumber}
\vspace{2cm}
\subsection{Rede convolucional com parâmetros compartilhados}

O código a seguir exibe o modelo de rede convolucional onde os parâmetros são compartilhados durante a aprendizagem das representações das questões e dos trechos de código-fonte. Diferente da Seção~\ref{sec:rede-convolucional-parametros-indepdentes-codigo-fonte-sem-normalizacao}, nas linhas 31 e 43, instanciamos apenas 1 camada do Conv1D e utilizamos este \textit{tensor} nas linhas seguintes (linhas 37,40 e linhas 49, 52).


\begin{mypython-linenumber}{Rede convolucional com parâmetros compartilhados}
class SharedConvolutionModel(LanguageModel):
    def build(self):
        assert self.config.question_len() == self.config.answer_len()

        question = Input(shape=(self.question_len,), dtype='int32', name='question')
        answer = Input(shape=(self.answer_len,), dtype='int32', name='answer')

        # add embedding layers
        question_weights = np.load(self.config.initial_question_weights())
        q_embedding = Embedding(input_dim=question_weights.shape[0],
                                output_dim=question_weights.shape[1],
                                weights=[question_weights],
                                name='question_embedding')
        question_embedding = q_embedding(question)

        answer_weights = np.load(self.config.initial_answer_weights())
        a_embedding = Embedding(input_dim=answer_weights.shape[0],
                                output_dim=answer_weights.shape[1],
                                weights=[answer_weights],
                                name='answer_embedding')
        answer_embedding = a_embedding(answer)

        # cnn
        filters = self.config.filters()
        kernel_size = self.kernel_size

        question_cnn = None
        answer_cnn = None

        if len(kernel_size) > 1:
            cnns = [Conv1D(kernel_size=k,
                           filters=filters,
                           activation='relu',
                           padding='same',
                           name=f'shared_conv1d_{k}') for k in kernel_size]
            # question_cnn = merge([cnn(question_embedding) for cnn in cnns], mode='concat')
            question_cnn = concatenate([cnn(question_embedding) for cnn in cnns])
            # answer_cnn = merge([cnn(answer_embedding) for cnn in cnns], mode='concat')

            answer_cnn = concatenate([cnn(answer_embedding) for cnn in cnns])
        else:
            k = kernel_size[0]
            cnn = Conv1D(kernel_size=k,
                           filters=filters,
                           activation='relu',
                           padding='same',
                           name=f'shared_conv1d_{k}')
            # question_cnn = merge([cnn(question_embedding) for cnn in cnns], mode='concat')
            question_cnn = cnn(question_embedding)
            # answer_cnn = merge([cnn(answer_embedding) for cnn in cnns], mode='concat')

            answer_cnn = cnn(answer_embedding)

        # maxpooling
        maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]),
                         name='max')
        maxpool.supports_masking = True
        # enc = Dense(100, activation='tanh')
        # question_pool = enc(maxpool(question_cnn))
        # answer_pool = enc(maxpool(answer_cnn))
        question_pool = maxpool(question_cnn)
        answer_pool = maxpool(answer_cnn)

        cos_similarity = Lambda(lambda x: cosine_similarity(x[0], x[1], axis=1)
                                       , output_shape=lambda _: (None, 1), name='similarity')([question_pool,
                                                                                               answer_pool])
        

        return Model(inputs=[question, answer], outputs=cos_similarity,
                                   name='qa_model')
\end{mypython-linenumber}

\vspace{2cm}
\subsection{Normalização em lote}

Durante os experimentos, verificamos que os modelos apresentavam sinais de \textit{overfitting}, onde havia um decréscimo rápido do erro na amostra de treinamento em comparação com o erro na amostra de validação (ver Seção~\ref{sec:regularizacao-normalizacao-lote}). Para mitigar este problema, criamos modelos que utilizam a normalização em lote. Durante a criação dos modelos, seguimos a recomendação dos próprios autores \cite{sergey-batch-normalization-2015} e adicionamos a normalização em lote antes da função de ativação.

\subsubsection{Rede convolucional com parâmetros independentes entre as camadas}

\begin{mypython-linenumber}{Rede convolucional com parâmetros independentes}
class ConvolutionModelWithBatchNormalization(LanguageModel):
    def build(self):
        assert self.config.question_len() == self.config.answer_len()

        question = Input(shape=(self.question_len,), dtype='int32', name='question')
        answer = Input(shape=(self.answer_len,), dtype='int32', name='answer')

        # add embedding layers
        question_weights = np.load(self.config.initial_question_weights())
        q_embedding = Embedding(input_dim=question_weights.shape[0],
                                output_dim=question_weights.shape[1],
                                weights=[question_weights],
                                name='question_embedding')
        question_embedding = q_embedding(question)

        answer_weights = np.load(self.config.initial_answer_weights())
        a_embedding = Embedding(input_dim=answer_weights.shape[0],
                                output_dim=answer_weights.shape[1],
                                weights=[answer_weights],
                                name='answer_embedding')
        answer_embedding = a_embedding(answer)

        # cnn
        filters = self.config.filters()
        kernel_size = self.kernel_size

        question_cnn = None
        answer_cnn = None


        if len(kernel_size) > 1:
            q_cnns = [Conv1D(kernel_size=k,
                             filters=filters,
                             padding='same',
                             name=f'question_conv1d_with_bn_{k}') for k in kernel_size]
            # question_cnn = merge([cnn(question_embedding) for cnn in cnns], mode='concat')

            question_outputs_cnn = [cnn(question_embedding) for cnn in q_cnns]
            bn_question_outputs_cnn = [BatchNormalization()(output) for output in question_outputs_cnn]
            activation_question_outputs_cnn = [Activation('relu')(output) for output in bn_question_outputs_cnn]

            question_cnn = concatenate([activation_output for activation_output in activation_question_outputs_cnn])

            # answer_cnn = merge([cnn(answer_embedding) for cnn in cnns], mode='concat')
            a_cnns = [Conv1D(kernel_size=k,
                             filters=filters,
                             padding='same',
                             name=f'answer_conv1d_with_bn_{k}') for k in kernel_size]

            answer_outputs_cnn = [cnn(answer_embedding) for cnn in a_cnns]
            bn_answer_outputs_cnn = [BatchNormalization()(output) for output in answer_outputs_cnn]
            activation_answer_outputs_cnn = [Activation('relu')(output) for output in bn_answer_outputs_cnn]

            answer_cnn = concatenate([activation_output for activation_output in activation_answer_outputs_cnn])
        else:
            k = kernel_size[0]
            q_cnn = Conv1D(kernel_size=k,
                             filters=filters,
                             padding='same',
                             name=f'question_conv1d_with_bn_{k}')
            # question_cnn = merge([cnn(question_embedding) for cnn in cnns], mode='concat')

            question_output_cnn = q_cnn(question_embedding)
            bn_question_output_cnn = BatchNormalization()(question_output_cnn)
            activation_question_output_cnn = Activation('relu')(bn_question_output_cnn)

            question_cnn = activation_question_output_cnn

            # answer_cnn = merge([cnn(answer_embedding) for cnn in cnns], mode='concat')
            a_cnn = Conv1D(kernel_size=k,
                             filters=filters,
                             padding='same',
                             name=f'answer_conv1d_with_bn_{k}')

            answer_output_cnn = a_cnn(answer_embedding)
            bn_answer_output_cnn = BatchNormalization()(answer_output_cnn)
            activation_answer_output_cnn = Activation('relu')(bn_answer_output_cnn)

            answer_cnn = activation_answer_output_cnn

        # maxpooling
        maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]),
                         name='max')
        maxpool.supports_masking = True
        # enc = Dense(100, activation='tanh')
        # question_pool = enc(maxpool(question_cnn))
        # answer_pool = enc(maxpool(answer_cnn))
        question_pool = maxpool(question_cnn)
        answer_pool = maxpool(answer_cnn)

        

        cos_similarity = Lambda(lambda x: cosine_similarity(x[0], x[1], axis=1)
                                       , output_shape=lambda _: (None, 1), name='similarity')([question_pool,
                                                                                               answer_pool])

        return Model(inputs=[question, answer], outputs=cos_similarity,
                                   name='qa_model')
\end{mypython-linenumber}
\vspace{2cm}
\subsubsection{Rede convolucional com parâmetros compartilhados}

\begin{mypython-linenumber}{Rede convolucional com parâmetros compartilhados e normalização em lote}
class SharedConvolutionModelWithBatchNormalization(LanguageModel):
    def build(self):
        assert self.config.question_len() == self.config.answer_len()

        question = Input(shape=(self.question_len,), dtype='int32', name='question')
        answer = Input(shape=(self.answer_len,), dtype='int32', name='answer')

        # add embedding layers
        question_weights = np.load(self.config.initial_question_weights())
        q_embedding = Embedding(input_dim=question_weights.shape[0],
                                output_dim=question_weights.shape[1],
                                weights=[question_weights],
                                name='question_embedding')
        question_embedding = q_embedding(question)

        answer_weights = np.load(self.config.initial_answer_weights())
        a_embedding = Embedding(input_dim=answer_weights.shape[0],
                                output_dim=answer_weights.shape[1],
                                weights=[answer_weights],
                                name='answer_embedding')
        answer_embedding = a_embedding(answer)

        # cnn
        filters = self.config.filters()
        kernel_size = self.kernel_size
        question_cnn = None
        answer_cnn = None

        if len(kernel_size) > 1:
            cnns = [Conv1D(kernel_size=k,
                           filters=filters,
                           padding='same',
                           name=f'shared_conv1d_with_bn_{k}') for k in kernel_size]
            # question_cnn = merge([cnn(question_embedding) for cnn in cnns], mode='concat')

            question_outputs_cnn = [cnn(question_embedding) for cnn in cnns]
            bn_question_outputs_cnn = [BatchNormalization()(output) for output in question_outputs_cnn]
            activation_question_outputs_cnn = [Activation('relu')(output) for output in bn_question_outputs_cnn]

            question_cnn = concatenate([activation_output for activation_output in activation_question_outputs_cnn])
            # answer_cnn = merge([cnn(answer_embedding) for cnn in cnns], mode='concat')

            answer_outputs_cnn = [cnn(answer_embedding) for cnn in cnns]
            bn_answer_outputs_cnn = [BatchNormalization()(output) for output in answer_outputs_cnn]
            activation_answer_outputs_cnn = [Activation('relu')(output) for output in bn_answer_outputs_cnn]

            answer_cnn = concatenate([activation_output for activation_output in activation_answer_outputs_cnn])
        else:
            k = kernel_size[0]
            cnn = Conv1D(kernel_size=k,
                           filters=filters,
                           padding='same',
                           name=f'shared_conv1d_with_bn_{k}')
            # question_cnn = merge([cnn(question_embedding) for cnn in cnns], mode='concat')

            question_output_cnn = cnn(question_embedding)
            bn_question_output_cnn = BatchNormalization()(question_output_cnn)
            activation_question_output_cnn = Activation('relu')(bn_question_output_cnn)

            question_cnn = activation_question_output_cnn
            # answer_cnn = merge([cnn(answer_embedding) for cnn in cnns], mode='concat')

            answer_output_cnn = cnn(answer_embedding)
            bn_answer_output_cnn = BatchNormalization()(answer_output_cnn)
            activation_answer_output_cnn = Activation('relu')(bn_answer_output_cnn)

            answer_cnn = activation_answer_output_cnn

        # maxpooling
        maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]),
                         name='max')
        maxpool.supports_masking = True
        # enc = Dense(100, activation='tanh')
        # question_pool = enc(maxpool(question_cnn))
        # answer_pool = enc(maxpool(answer_cnn))
        question_pool = maxpool(question_cnn)
        answer_pool = maxpool(answer_cnn)

        cos_similarity = Lambda(lambda x: cosine_similarity(x[0], x[1], axis=1)
                                       , output_shape=lambda _: (None, 1), name='similarity')([question_pool,
                                                                                               answer_pool])

        

        return Model(inputs=[question, answer], outputs=cos_similarity,
                                   name='qa_model')
\end{mypython-linenumber}

