\newglossaryentry{ml}
{
    name=aprendizagem de máquina,
    description={sistema ou programa que constrói um modelo preditivo a partir de dados de entrada \citep{glossary-ml}}
}

\newglossaryentry{modelo}
{
    name=modelo,
    description={Representação do que um sistema de \gls{ml} aprendeu a partir de dados de treinamento \citep{glossary-ml}}
}

\newglossaryentry{sof}
{
    name=StackOverflow,
    description={Site de perguntas e respostas de programação. Endereço do site: \url{https://www.stackoverflow.com/}}
}

\newglossaryentry{git}
{
    name=git,
    description={git é um versionador de controle distribuído para rastrear alterações no código-fonte durante o desenvolvimento de software. Endereço do site: \url{https://git-scm.com/} \citep{wikipedia-git-2019}}
}

\newglossaryentry{github}
{
    name=GitHub,
    description={GitHub é uma plataforma de hospedagem de código-fonte com controle de versão usando o \gls{git}. Endereço do site: \url{https://www.github.com/}}
}

\newglossaryentry{representacao-distribuida}
{
    name=representação distribuída,
    description={Representação distribuída significa uma relação de muitos para muitos entre dois tipos de representações (por exemplo, conceitos e \gls{neuron}s) \citep{Hinton-distributed-representatons:1986}. 
    \begin{itemize}
        \item Cada conceito é representado por muitos \gls{neuron}s
        \item Cada \gls{neuron} participa na representação de muitos conceitos
    \end{itemize}
    }
}

\newglossaryentry{one-hot-encoding}
{
    name=\textit{one-hot encoding},
    description={\textit{one-hot encoding} é um vetor esparso que contém:
    \begin{itemize}
        \item Um elemento cujo valor é definido como 1
        \item O restante dos elementos tem o valor definido como 0
    \end{itemize}
    \textit{One-hot enconding} normalmente é utilizado para representar palavras e ou atributos que contém uma quantidade finita de valores \citep{glossary-ml}
    }
}

\newglossaryentry{neuron}
{
    name=neurônio,
    description={Um neurônio é um nó numa rede neural, que tipicamente recebe múltiplos valores de entrada e gera um valor de resultado. O neurônio aplica uma função de ativação (transformação não-linear) na soma dos valores de entrada com seus respectivos pesos \citep{glossary-ml}}
}

\newglossaryentry{bag-of-words}
{
    name=bag of words,
    description={Vetor composto por palavras indiferente à ordem e permutação. Ver exemplo na Seção~\ref{sec:representacao-das-sentencas-fundamentacao-teorica}}
}

\newglossaryentry{mecanismo-atencao}{
name=mecanismo de atenção,
description={
    O mecanismo de atenção calcula a média ponderada dos elementos de um vetor e o principal objetivo dele é encontrar um peso para cada elemento. Ao aplicarmos o mecanismo em uma tarefa de tradução, por exemplo, a cada momento que uma palavra é traduzida, o mecanismo de atenção foca em partes diferentes da sentença, i.e, ele aprende a ''prestar atenção'' nas palavras mais relevantes \citep{Goodfellow-et-al-2016}
}}

\newglossaryentry{docstring}{
name=docstring,
description={
    Em programação, um \textit{docstring} é um texto especificado no código-fonte que é usado para documentar um trecho específico do código \cite{wikipedia-docstring-2019}
}}

\newglossaryentry{jupyter}{
name=Jupyter,
description={
    \textit{Jupyter Notebook} é uma ferramenta interativa que permite desevolver, executar e documentar código em uma aplicação web. O termo \textit{notebook} refere-se a um caderno de anotações, pois é possível desenvolver, salvar as saídas do programa e fazer anotações \cite{jupyter-2019}
}}

\newglossaryentry{colab}{
name=Colab,
description={
    É uma ferramenta de pesquisa e ensino para aprendizagem de máquina. É um ambiente \Gls{jupyter} que não necessita configuração ou instalação \cite{colab-2019}
}}

\newglossaryentry{unif}{
name=Unif,
description={
    Arquitetura de rede neural com mecanismo de atenção proposta por \cite{cambronero-deep-learning-code-search:2019} para a recuperação de trecho de código-fonte
}}

\newglossaryentry{xlnet}{
name=XLNet,
description={
    Arquitetura autoregressiva para compreensão de linguagem durante o pré-treinamento \citep{yang2019xlNet}
}}

\newglossaryentry{word2vec}{
name=Word2vec,
description={
    Word2vec é uma técnica para representar palavras através de vetores de \gls{representacao-distribuida}. Mais informações na Seção~\ref{sec:fundamentao-representacao-tokens-palavras}
}}

\newglossaryentry{tensorflow}{
name=Tensorflow,
description={
    Tensorflow é uma biblioteca aberta de aprendizagem de máquina aplicável a uma ampla variedade de tarefas \citep{wikipedia-tensorflow-2020}
}}

\newglossaryentry{keras}{
name=Keras,
description={
    Keras é uma biblioteca aberta de redes neurais escrita em Python \citep{wikipedia-keras-2020}
}}




\newacronym{ide}{IDE}{Integrated Development Environment}

\newacronym{rnn}{RNN}{Recurrent Neural Network}
\newacronym{lstm}{LSTM}{Long Short Term Memory}
\newacronym{nlp}{NLP}{Processamento de Linguagem Natural}
\newacronym{vae}{VAE}{Variational AutoEncoder}
\newacronym{tf-idf}{TFIDF}{Term Frequency–Inverse Document
Frequency}
\newacronym{cbow}{CBoW}{Comsuption Bag of Words}
\newacronym{sof-ab}{SO}{Stack Overflow}
\newacronym{github-ab}{GH}{GitHub}
\newacronym{cnn}{CNN}{Convolutional Neural Network}
\newacronym{vem}{VEM}{Workshop on Software Visualization, Evolution and Maintenance}
\newacronym{mrr}{MRR}{Mean Reciprocal Rank}
\newacronym{vgpu}{vGPU}{Virtual Graphics Processing Unit}
\newacronym{elmo}{ELMo}{Embeddings from Language Models}
\newacronym{bert}{BERT}{Bidirection Encoder Representations from Transformers}
\newacronym{squad}{SQuAD}{Stanford Question Answering Dataset}
\newacronym{map}{MAP}{Mean Average Precision}
\newacronym{ndcg}{NDCG}{Normalized Discounted Cumulative Gain}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{cpu}{CPU}{Central Processing Unit}










